{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX + Random Walk Metropolis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpUGG0Kuu56BvWrrjcyxxG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlouf/blog-benchmark-rwmetropolis/blob/master/notebooks/gpu_random_walk_metropolis_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pPUrBBwZojeJ",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax.scipy.stats import norm\n",
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "import numpy as onp\n",
        "\n",
        "# don't do this at home, warnings are there for a reason\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAz5IoSxoj5w",
        "colab_type": "text"
      },
      "source": [
        "This notebook accompanies a [blog post](https://rlouf.github.io/post/jax-random-walk-metropolis/) I wrote on the performance of vectorized sampling with the Random Walk Metropolis algorithm. I compared the performance of Numpy, JAX and Tensorflow Probability on CPU. The response was overwhelming, and [Matthew Johnson](https://twitter.com/SingularMattrix) and [Hector Yee](https://twitter.com/eigenhector) were kind enough to point out that I fell into JAX's pseudo-random generator trap, and thus greatly overestimated JAX's performance. Don't make the same mistake, and read the [doc](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers) carefully before playing with random numbers with JAX.\n",
        "\n",
        "Finally, [Erwin Coumans](https://twitter.com/erwincoumans) suggested on Twitter that I turn part of this benchmark into a notebook Colab to give people a point of comparison with GPU and TPUs. This is a excellent idea, so here it is.\n",
        "\n",
        "It unfortunately turns out that I am not smart enough to make the equivalent in TFP run on GPU/TPU, any help would be appreciated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUvKlkUN_a4M",
        "colab_type": "text"
      },
      "source": [
        "# The setup\n",
        "\n",
        "The basic requirements to be able to generate samples is the transition kernel of the random walk, and a log-probability density function to sample from. I chose a completely arbitrary gaussian mixture with 4 components.\n",
        "\n",
        "There are a couple of interesting things going on already:\n",
        "\n",
        "- The kernel is written for a single chain. We will use JAX's `vmap` function to vectorize the computation. You can find the doc [here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#Auto-vectorization-with-vmap).\n",
        "- `jax.numpy` acts as a drop-in replacement to `numpy`.\n",
        "- `jax.random.split` is, roughly speaking, the function that allows you to \"advance\" the random number generator. If you don't call this function you will be always using the same number. This is hat I did not understand the first time.\n",
        "- The `jax.jit` decorator tells JAX that you want the function to be JIT compiled. `static_argnums` tells the compiler which parameters of the function will not change when the function is called repeatedly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NAa--T5VnEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@partial(jax.jit, static_argnums=(1,))\n",
        "def rw_metropolis_kernel(rng_key, logpdf, position, log_prob):\n",
        "    \"\"\"Moves a single chain by one step using the Random Walk Metropolis algorithm.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    rng_key: jax.random.PRNGKey\n",
        "      Key for the pseudo random number generator.\n",
        "    logpdf: function\n",
        "      Returns the log-probability of the model given a position.\n",
        "    position: np.ndarray, shape (n_dims,)\n",
        "      The starting position.\n",
        "    log_prob: float\n",
        "      The log probability at the starting position.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple\n",
        "        The next positions of the chains along with their log probability.\n",
        "    \"\"\"\n",
        "    key1, key2 = jax.random.split(rng_key)\n",
        "    move_proposal = jax.random.normal(key1, shape=position.shape) * 0.1\n",
        "    proposal = position + move_proposal\n",
        "    proposal_log_prob = logpdf(proposal)\n",
        "\n",
        "    log_uniform = np.log(jax.random.uniform(key2))\n",
        "    do_accept = log_uniform < proposal_log_prob - log_prob\n",
        "\n",
        "    position = np.where(do_accept, proposal, position)\n",
        "    log_prob = np.where(do_accept, proposal_log_prob, log_prob)\n",
        "    return position, log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8imh07TftjLA",
        "colab_type": "text"
      },
      "source": [
        "`jax.scipy` acts as a drop-in replacement to `scipy` functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x_0YFoHV8IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mixture_logpdf(x):\n",
        "    \"\"\"Log probability distribution function of a gaussian mixture model.\n",
        "\n",
        "    Attribute\n",
        "    ---------\n",
        "    x: np.ndarray (4,)\n",
        "        Position at which to evaluate the probability density function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The value of the log probability density function at x.\n",
        "    \"\"\"\n",
        "    dist_1 = jax.partial(norm.logpdf, loc=-2.0, scale=1.2)\n",
        "    dist_2 = jax.partial(norm.logpdf, loc=0, scale=1)\n",
        "    dist_3 = jax.partial(norm.logpdf, loc=3.2, scale=5)\n",
        "    dist_4 = jax.partial(norm.logpdf, loc=2.5, scale=2.8)\n",
        "    log_probs = np.array([dist_1(x), dist_2(x), dist_3(x), dist_4(x)])\n",
        "    weights = np.array([0.2, 0.3, 0.1, 0.4])\n",
        "    return -logsumexp(np.log(weights) + log_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjTPNNXbWOZ1",
        "colab_type": "text"
      },
      "source": [
        "# Sampling the posterior\n",
        "\n",
        "This post was originally a simple sanity check before working on a larger project where I am only interested in the last sample that was obtained for each chain. I therefore use the `lax.fori_loop` construct. If you want want to work with all the samples that were produced, use the `lax.scan` construct instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOl0pWjzV0Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@partial(jax.jit, static_argnums=(1, 2))\n",
        "def rw_metropolis_sampler(rng_key, n_samples, logpdf, initial_position):\n",
        "    \"\"\"Generate samples using the Random Walk Metropolis algorithm.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    rng_key: jax.random.PRNGKey\n",
        "        Key for the pseudo random number generator.\n",
        "    n_samples: int\n",
        "        Number of samples to generate per chain.\n",
        "    logpdf: function\n",
        "      Returns the log-probability of the model given a position.\n",
        "    inital_position: np.ndarray (n_dims, n_chains)\n",
        "      The starting position.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (n_samples, n_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def sampler_step(i, state):\n",
        "        key, position, log_prob = state\n",
        "        _, key = jax.random.split(key)\n",
        "        new_position, new_log_prob = rw_metropolis_kernel(key, logpdf, position, log_prob)\n",
        "        return (key, new_position, new_log_prob)\n",
        "\n",
        "    logp = logpdf(initial_position)\n",
        "    rng_key, position, log_prob = jax.lax.fori_loop(0, n_samples, sampler_step, (rng_key, initial_position, logp))\n",
        "    return position"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DohySX4svJna",
        "colab_type": "text"
      },
      "source": [
        "And we define the function that will initialize and run the sampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P5xFHV7vYB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_jax(rng_key, logpdf, n_dim, n_samples, n_chains):\n",
        "    rng_keys = jax.random.split(rng_key, n_chains)  # (nchains,)\n",
        "    initial_position = np.zeros((n_dim, n_chains))  # (n_dim, n_chains)\n",
        "    run_mcmc = jax.vmap(rw_metropolis_sampler, in_axes=(0, None, None, 1),\n",
        "                        out_axes=1)\n",
        "    positions = run_mcmc(rng_keys, n_samples, logpdf, initial_position).block_until_ready()\n",
        "    assert positions.shape == (n_dim, n_chains)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHzq2o4RVJKl",
        "colab_type": "text"
      },
      "source": [
        "## Drawing 1,000 samples for an increasing number of chains "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf5je4uoVC_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_dim=4\n",
        "n_samples = 1_000\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "\n",
        "chain_lengths = onp.logspace(1, 7, 7)\n",
        "chains_exec_times = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtsL5VBuWARo",
        "colab_type": "code",
        "outputId": "13cdcfdf-9a99-4fbd-c6c4-57938af57dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "for n_chains in chain_lengths:\n",
        "    n_chains = int(n_chains)\n",
        "    t = %timeit -o sample_jax(rng_key, mixture_logpdf, n_dim, n_samples, n_chains)\n",
        "    avg = sum(t.all_runs)/(len(t.all_runs))\n",
        "    chains_exec_times.append(avg)\n",
        "    print(\"{:,} chains: {:.3} s\".format(n_chains, avg))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 78.59 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 27.3 ms per loop\n",
            "10 chains: 0.0281 s\n",
            "The slowest run took 54.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 27.2 ms per loop\n",
            "100 chains: 0.0276 s\n",
            "The slowest run took 50.15 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 31.3 ms per loop\n",
            "1,000 chains: 0.0324 s\n",
            "The slowest run took 43.02 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 36.3 ms per loop\n",
            "10,000 chains: 0.0366 s\n",
            "The slowest run took 13.16 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 127 ms per loop\n",
            "100,000 chains: 0.128 s\n",
            "1 loop, best of 3: 1.1 s per loop\n",
            "1,000,000 chains: 1.1 s\n",
            "1 loop, best of 3: 10.7 s per loop\n",
            "10,000,000 chains: 10.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZhvnw64zBIV",
        "colab_type": "text"
      },
      "source": [
        "## Drawing an increasing number of samples for 1,000 chains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f1l86iMWWmop",
        "colab": {}
      },
      "source": [
        "n_dim=4\n",
        "n_chains = 1_000\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "\n",
        "samples_num = onp.logspace(1, 6, 6)\n",
        "samples_exec_times = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxjQyI1RzK-_",
        "colab_type": "code",
        "outputId": "99691ade-330d-494a-81b5-771d4717a866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for n_samples in samples_num:\n",
        "    n_samples = int(n_samples)\n",
        "    t = %timeit -o sample_jax(rng_key, mixture_logpdf, n_dim, n_samples, n_chains)\n",
        "    avg = sum(t.all_runs)/(len(t.all_runs))\n",
        "    samples_exec_times.append(avg)\n",
        "    print(\"{:,} samples: {:.3} s\".format(n_samples, avg))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 927.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 1.26 ms per loop\n",
            "10 samples: 0.00143 s\n",
            "The slowest run took 301.72 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 3.75 ms per loop\n",
            "100 samples: 0.00387 s\n",
            "10 loops, best of 3: 32.3 ms per loop\n",
            "1,000 samples: 0.324 s\n",
            "The slowest run took 4.91 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 3: 301 ms per loop\n",
            "10,000 samples: 0.302 s\n",
            "1 loop, best of 3: 3 s per loop\n",
            "100,000 samples: 3.01 s\n",
            "1 loop, best of 3: 30 s per loop\n",
            "1,000,000 samples: 30.0 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}